{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\myrepos\\Audio_inpainting\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioned-U-Net: Introducing a Control Mechanism in the U-Net for Multiple Source Separations\n",
    "\n",
    "[0] Meseguer-Brocal, Gabriel, and Geoffroy Peeters. \"CONDITIONED-U-NET: INTRODUCING A CONTROL MECHANISM IN THE U-NET FOR MULTIPLE SOURCE SEPARATIONS.\" Proceedings of the 20th International Society for Music Information Retrieval Conference. 2019.\n",
    "\n",
    "### Abstract\n",
    "\n",
    "Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically **trained for a single task**, e.g. a **particular instrument isolation**. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the **Conditioned-U-Net (C-U-Net)** which adds a **control mechanism** to the standard **U-Net**. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The C-U-Net decides the instrument to isolate according to a one-hot-encoding input vector. The input vector is embedded to obtain the parameters that control **Feature-wise Linear Modulation (FiLM)** layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Motivation of Conditioned U-Net\n",
    "\n",
    "![](https://imgur.com/2s7l5O0.png)\n",
    "\n",
    "Figure from [0]: Traditional Approach VS Conditioned-U-Net\n",
    "\n",
    "```\n",
    "This control mechanism permits multiple instrument source separations with a single model without losing any performance.\n",
    "It is motivated by the idea that we can process the same input differently depending of some external context.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Core Idea\n",
    "\n",
    "See more details from this link https://github.com/gabolsgabs/cunet\n",
    "\n",
    "![](https://github.com/gabolsgabs/cunet/raw/master/.markdown_images/overview.png)\n",
    "\n",
    "\n",
    "#### 2.0. Overview\n",
    "\n",
    "```\n",
    "\n",
    "two core elements:\n",
    "\n",
    "- **Generic model**: not specialized in any particular task, but rather in finding a set of generic source separation tools.\n",
    "- **Control**: that defines how to combine the set of tools.\n",
    "```\n",
    "\n",
    "![](https://imgur.com/3fYhGAi.png)\n",
    "\n",
    "#### 2.1. Generic model\n",
    "\n",
    "```\n",
    "The generic model is a U-Net[3] and it has to main part:\n",
    "- **The decoder** codifies and highlights the relevant information to separate a particular instrument creating a latent space.\n",
    "- **The encoder** transforms the latent space back to audio signal.\n",
    "\n",
    "It is important to decide where to condition. And we think that it is essential to be able to create **different latent spaces per instrument**. Therefore, we condition **only the decoder**.\n",
    "\n",
    "**FiLM layers[4]** are applied at each encoder block after batch normalization.\n",
    "```\n",
    "\n",
    "#### 2.2. Condition generator\n",
    "\n",
    "```\n",
    "The condition generator takes as input a vector with the desired instrument to separate and compute the gammas and betas that control the generic model. We test two different configurations based on fully-connected architecture and convolutional one.  \n",
    "\n",
    "Both systems are trained jointly.\n",
    "\n",
    "```\n",
    "\n",
    "##### 2.2.a. Control Model\n",
    "\n",
    "```python\n",
    "class dense_control_model(pl.LightningModule):\n",
    "    def __init__(self, dense_control_block, output_features, split,\n",
    "                 gamma_activation=nn.Identity, beta_activation=nn.Identity):\n",
    "        super(dense_control_model, self).__init__()\n",
    "        self.dense_control_block = dense_control_block\n",
    "        self.split = split\n",
    "        self.gamma_activation = gamma_activation()\n",
    "        self.beta_activation = beta_activation()\n",
    "\n",
    "        self.linear_gamma = nn.Sequential(\n",
    "            nn.Linear(dense_control_block.last_dim, output_features),\n",
    "            self.gamma_activation\n",
    "        )\n",
    "        self.linear_beta = nn.Sequential(\n",
    "            nn.Linear(dense_control_block.last_dim, output_features),\n",
    "            self.beta_activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a one-hot encoded vector\n",
    "        x = self.dense_control_block(x) \n",
    "        # x is a latent space vector that encodes one-hot encoded condition\n",
    "        \n",
    "        # we split the output to distribute it into each encoding layer.  \n",
    "        return self.split(self.linear_gamma(x)), self.split(self.linear_beta(x))\n",
    "\n",
    "```\n",
    "##### 2.2.b. FiLM Layer\n",
    "\n",
    "```\n",
    "FiLM permits to modulate any neural network architecture inserting one or several FiLM layers at any depth of the original model [4]:\n",
    "```\n",
    "\n",
    "- $ FiLM(x) = \\gamma(z) \\cdot x + \\beta(z) $\n",
    "\n",
    "```\n",
    "x is the input of the FiLM layer and gamma and beta the learnable parameters that scale and shift x based on an external information, z.\n",
    "\n",
    "The original FiLM computes a different operation per features map, and we propose a new layer that performs the same affine operation to the whole input.\n",
    "```\n",
    "\n",
    "![](https://github.com/gabolsgabs/cunet/raw/master/.markdown_images/FiLM_layers.png)\n",
    "\n",
    "\n",
    "Below is an implementation of FiLM operation:\n",
    "\n",
    "```python\n",
    "def FiLM_simple(x, gamma, beta):\n",
    "    \"\"\"\n",
    "    :param x: an output feature map of a CNN layer [*, ch, T, F]\n",
    "    :param gamma: [*, 1]\n",
    "    :param beta: [*, 1]\n",
    "    :return: gamma * x + beta\n",
    "    \"\"\"\n",
    "    gamma_ = gamma.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    beta_ = beta.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    return gamma_ * x + beta_\n",
    "\n",
    "\n",
    "def FiLM_complex(x, gammas, betas):\n",
    "    \"\"\"\n",
    "    :param x: an output feature map of a CNN layer [*, ch, T, F]\n",
    "    :param gamma: [*, ch]\n",
    "    :param beta: [*, ch]\n",
    "    :return: gamma * x + beta\n",
    "    \"\"\"\n",
    "    gamma_ = gammas.unsqueeze(-1).unsqueeze(-1)\n",
    "    beta_ = betas.unsqueeze(-1).unsqueeze(-1)\n",
    "    return gamma_ * x + beta_\n",
    "```\n",
    "\n",
    "\n",
    "##### 2.2.c. (Merge) Encoding Layers of Conditioned-U-Net\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "from ielab_audio.models.building_blocks import TIF, TFC\n",
    "from ielab_audio.utils.weight_initialization import WI_Module\n",
    "\n",
    "class cu_net_conv_block(WI_Module):\n",
    "    def __init__(self, conv_layer, bn_layer, film_layer, activation):\n",
    "        super(cu_net_conv_block, self).__init__()\n",
    "        self.bn_layer = bn_layer\n",
    "        self.conv_layer = conv_layer\n",
    "        self.FiLM_layer = film_layer\n",
    "        self.activation = activation\n",
    "        self.in_channels = self.conv_layer.conv.in_channels\n",
    "        self.out_channels = self.conv_layer.conv.out_channels\n",
    "\n",
    "    def forward(self, x, gamma, beta):\n",
    "        x = self.bn_layer(self.conv_layer(x))\n",
    "        x = self.FiLM_layer(x, gamma, beta)\n",
    "        return self.activation(x)\n",
    "\n",
    "```\n",
    "\n",
    "[3] *A.Jansson, N.Montecchio, R.Bittner, A.Kumar, T.Weyde, E. J. Humphrey. Singing voice separation with deep u-net convolutional networks. In Proc. of ISMIR (International Society for Music Information Retrieval), Suzhou, China, 2017.*\n",
    "\n",
    "[4] *E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. C. Courville. Film: Visual reasoning with a general condition- ing layer. In Proc. of AAAI (Conference on Artificial Intelligence), New Orleans, LA, USA, 2018.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implemantation\n",
    "\n",
    "```python\n",
    "class CUNET(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 input_channels,\n",
    "                 filters_layer_1,\n",
    "                 kernel_size=(5, 5),\n",
    "                 stride=(2, 2),\n",
    "                 film_type='simple',\n",
    "                 control_type='dense',\n",
    "                 encoder_activation=nn.LeakyReLU,\n",
    "                 decoder_activation=nn.ReLU,\n",
    "                 last_activation=nn.Sigmoid,\n",
    "                 control_input_dim=4,\n",
    "                 control_n_layer=4\n",
    "                 ):\n",
    "\n",
    "        encoder_activation = get_activation_by_name(encoder_activation)\n",
    "        decoder_activation = get_activation_by_name(decoder_activation)\n",
    "        self.last_activation = last_activation = get_activation_by_name(last_activation)\n",
    "\n",
    "        super(CUNET, self).__init__()\n",
    "\n",
    "        self.input_control_dims = control_input_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.input_channels = input_channels\n",
    "        self.filters_layer_1 = filters_layer_1\n",
    "\n",
    "        # Encoder\n",
    "        encoders = []\n",
    "        for i in range(n_layers):\n",
    "            output_channels = filters_layer_1 * (2 ** i)\n",
    "            encoders.append(\n",
    "                cu_net_conv_block(\n",
    "                    conv_layer=Conv2d_same(input_channels, output_channels, kernel_size, stride),\n",
    "                    bn_layer=nn.BatchNorm2d(output_channels),\n",
    "                    film_layer=FiLM_simple if film_type == \"simple\" else FiLM_complex,\n",
    "                    activation=encoder_activation()\n",
    "                )\n",
    "            )\n",
    "            input_channels = output_channels\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "\n",
    "        # Decoder\n",
    "        decoders = []\n",
    "        for i in range(n_layers):\n",
    "            # parameters each decoder layer\n",
    "            is_final_block = i == n_layers - 1  # the las layer is different\n",
    "            # not dropout in the first block and the last two encoder blocks\n",
    "            dropout = not (i == 0 or i == n_layers - 1 or i == n_layers - 2)\n",
    "            # for getting the number of filters\n",
    "            encoder_layer = self.encoders[n_layers - i - 1]\n",
    "            skip = i > 0  # not skip in the first encoder block\n",
    "\n",
    "            input_channels = encoder_layer.out_channels\n",
    "            if skip:\n",
    "                input_channels *= 2\n",
    "\n",
    "            if is_final_block:\n",
    "                output_channels = self.input_channels\n",
    "                activation = last_activation\n",
    "            else:\n",
    "                output_channels = encoder_layer.in_channels\n",
    "                activation = decoder_activation\n",
    "\n",
    "            decoders.append(\n",
    "                u_net_deconv_block(\n",
    "                    deconv_layer=ConvTranspose2d_same(input_channels, output_channels, kernel_size, stride),\n",
    "                    bn_layer=nn.BatchNorm2d(output_channels),\n",
    "                    activation=activation(),\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.decoders = nn.ModuleList(decoders)\n",
    "\n",
    "        # Control Mechanism\n",
    "        if film_type == \"simple\":\n",
    "            control_output_dim = n_layers\n",
    "\n",
    "            def split_simple(tensor):\n",
    "                return [tensor[..., layer] for layer in range(n_layers)]\n",
    "\n",
    "            split = split_simple\n",
    "\n",
    "        else:\n",
    "            output_channel_array = [encoder.conv_layer.conv.out_channels for encoder in self.encoders]\n",
    "            control_output_dim = sum(output_channel_array)\n",
    "\n",
    "            start_idx_per_layer = [sum(output_channel_array[:i]) for i in range(len(output_channel_array))]\n",
    "            end_idx_per_layer = [sum(output_channel_array[:i + 1]) for i in range(len(output_channel_array))]\n",
    "\n",
    "            def split_complex(tensor):\n",
    "                return [tensor[..., start:end] for start, end in\n",
    "                        zip(start_idx_per_layer, end_idx_per_layer)]\n",
    "\n",
    "            split = split_complex\n",
    "\n",
    "        if control_type == \"dense\":\n",
    "            self.condition_generator = dense_control_model(\n",
    "                dense_control_block(control_input_dim, control_n_layer),\n",
    "                control_output_dim,\n",
    "                split\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def init_weights(self):\n",
    "        for sub_module in self.encoders:\n",
    "            sub_module.init_weights()\n",
    "        for sub_module in self.decoders:\n",
    "            sub_module.init_weights()\n",
    "        self.condition_generator.init_weights()\n",
    "\n",
    "    def forward(self, input_spec, input_condition):\n",
    "\n",
    "        gammas, betas = self.condition_generator(input_condition)\n",
    "\n",
    "        x = input_spec\n",
    "\n",
    "        # Encoding Phase\n",
    "        encoder_outputs = []\n",
    "        for encoder, gamma, beta in zip(self.encoders, gammas, betas):\n",
    "            encoder_outputs.append(encoder(x, gamma, beta))  # TODO\n",
    "            x = encoder_outputs[-1]\n",
    "\n",
    "        # Decoding Phase\n",
    "        x = self.decoders[0](x)\n",
    "        for decoder, x_encoded in zip(self.decoders[1:], reversed(encoder_outputs[:-1])):\n",
    "            x = decoder(torch.cat([x, x_encoded], dim=-3))\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n",
    "![](https://imgur.com/3fYhGAi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation\n",
    "\n",
    "might need to be improved :(\n",
    "\n",
    "\n",
    "#### 4.1 Original Paper\n",
    "![](imgs/table2.png)\n",
    "![](imgs/table3.png)\n",
    "\n",
    "#### Our implementation\n",
    "\n",
    "|Name                     |control_input_dim|control_n_layer|control_type|decoder_activation|encoder_activation|film_type|filters_layer_1|hop_length|input_channels|kernel_size|last_activation|lr   |n_fft|n_layers|num_frame|optimizer|stride|test_result/agg/bass_ISR|test_result/agg/bass_SAR|test_result/agg/bass_SDR|test_result/agg/bass_SIR|test_result/agg/drums_ISR|test_result/agg/drums_SAR|test_result/agg/drums_SDR|test_result/agg/drums_SIR|test_result/agg/other_ISR|test_result/agg/other_SAR|test_result/agg/other_SDR|test_result/agg/other_SIR|test_result/agg/vocals_ISR|test_result/agg/vocals_SAR|test_result/agg/vocals_SDR|test_result/agg/vocals_SIR|\n",
    "|-------------------------|-----------------|---------------|------------|------------------|------------------|---------|---------------|----------|--------------|-----------|---------------|-----|-----|--------|---------|---------|------|------------------------|------------------------|------------------------|------------------------|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n",
    "|complex_2048_512_128eval |4                |4              |dense       |relu              |leaky_relu        |complex  |24             |512       |2             |[5,5]      |sigmoid        |0.001|2048 |6       |128      |adam     |[2,2] |8.84835                 |4.81325                 |2.795465                |4.114615                |9.69044                  |4.2979225                |3.492365                 |4.63526                  |6.93455                  |3.87871                  |1.85376                  |1.0855625                |6.0647475                 |2.2080925                 |2.49749                   |8.3487875                 |\n",
    "|complex_32eval_          |4                |4              |dense       |relu              |leaky_relu        |complex  |32             |256       |2             |[5,5]      |sigmoid        |0.001|1024 |6       |256      |adam     |[2,2] |8.0865575               |4.79529                 |2.1145975               |2.6459025               |10.019905                |4.9158075                |3.795275                 |4.92333                  |7.5122025                |4.58683                  |1.705415                 |1.07406                  |7.470695                  |3.63371                   |2.415865                  |6.5487125                 |\n",
    "|cunet_mme_sigmoid_32-eval|4                |4              |dense       |relu              |leaky_relu        |simple   |32             |256       |2             |[5,5]      |sigmoid        |0.001|1024 |6       |256      |adam     |[2,2] |7.6462525               |4.90194                 |1.84956                 |1.9313625               |9.4997225                |4.6694725                |3.327125                 |4.113235                 |7.648405                 |4.659825                 |1.500495                 |0.5541025                |6.710985                  |3.602105                  |2.12235                   |5.72728                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Discussion\n",
    "\n",
    "- Author's Assumptions\n",
    "    - This control mechanism permits multiple instrument source separations with a single model without **losing any performance**.\n",
    "    - It is important to decide where to condition. And we think that it is essential to be able to create **different latent spaces per instrument**. Therefore, we condition **only the decoder**.    \n",
    "    - Spectrogram-independent Modulation VS Modulation that both considers input conditions and spectrograms\n",
    "        - The C-U-net is more likely a two-streamed model.\n",
    "    - One-hot encoding input vector\n",
    "\n",
    "- Room for improvement\n",
    "    - [TFC_TDF_CUnet-complex](https://app.wandb.ai/wschoi/round_1?workspace=user-wschoi)\n",
    "    - [TFC_TDF_CUnet-simple](https://app.wandb.ai/wschoi/source_separation?workspace=user-wschoi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
