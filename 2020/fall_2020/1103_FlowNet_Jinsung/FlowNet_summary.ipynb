{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlowNet : Learning Optical Flow ith Convolutional Networks\n",
    "\n",
    "`Philipp Fishcher`\n",
    "`Alexey Dosovitskiy`\n",
    "\n",
    "# FlowNet 2.0 : Evolution of Optical Flow Estimation with Deep Networks\n",
    "`Eddy Ilg`\n",
    "`Nikolaus Mayer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. OverView\n",
    "\n",
    "## FlowNet\n",
    "\n",
    "optical flow를 convolution neural net으로 추측하는 model\n",
    "![FlowNet](img/07_1.png)\n",
    "![FlowNet](img/07_2.png)\n",
    "## FlowNet2.0\n",
    "\n",
    "![FlowNet2](img/07_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Optical Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Optical Flow(광학흐름)\n",
    "\n",
    "광학흐름(optical flow)는 어떤 물체의 표면(surface)혹은 모서리(edge)의 움직임(Motion)을 나타내는 Vector Map을 말한다. \n",
    "\n",
    "---\n",
    "![optical_flow](img/01_1.png)\n",
    "    <center>출처 : [http://of-eval.sourceforge.net/](http://of-eval.sourceforge.net/)</center>\n",
    "\n",
    "왼쪽 이미지에서 오른쪽 이미지로의 움직임(회전)을 가지는 두 이미지에서의 optical flow는 가운데와 같이 나타 낼 수 있다.\n",
    "\n",
    "이미지에서의 optical flow는 각 pixcel당 x,y방향의 벡터값을 가진다. 2d voxcel과 같다.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "화살표로 optical flow를 표현할때, 모든 픽셀에 대해 표현하면 보기가 힘들기 때문에 주로 color map을 이용하여 표현한다. \n",
    "\n",
    "![OpticalFlow_ColorMap](img/02.png)\n",
    "\n",
    "\n",
    "![03_00](img/03_00.png)\n",
    "![03_01](img/03_01.png)\n",
    "![03_02](img/03_02.png)\n",
    "<center>출처 : KITTI data set</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Basic Optical Flow Estimation\n",
    "\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Optical_flow)를 참고하여 작성하였습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Optical Flow를 기본적인 컨셉은 다음과 같다.\n",
    "\n",
    "- $t$시간일 때, $x,y$ 위치에 있는 image의 값을 $I(x,y,t)$라고 할 때, $\\Delta{t}$ 동안 $\\Delta{x},\\Delta{y}$만큼 움직였을 때, 밝기는 변하지 않고 위치만 움직였다고 가정하면 다음과 같이 표현 할 수 있다.\n",
    "\n",
    "![](img/04_01.png)\n",
    "\n",
    "- 여기서 $I(x,y,t)$를 Taylor series로 표현하면, \n",
    "\n",
    "![](img/04_02.png)\n",
    "\n",
    "- Higher order term을 무시하고, $I(x,y,t) = I(x+\\Delta{x}, y+\\Delta{y}, t+\\Delta{t})$ 이므로.\n",
    "\n",
    "![](img/04_03.png)\n",
    "\n",
    "- $\\Delta{t}$로 나누어주면.\n",
    "\n",
    "![](img/04_04.png)\n",
    "\n",
    "- $ V(x) = \\frac{\\Delta{x}}{ \\Delta{t}}, V(y) = \\frac{ \\Delta{y} }{ \\Delta{x}}$ 로 바꾸면\n",
    "\n",
    "![](img/04_05.png)\n",
    "\n",
    "- 결국 각각 Image의 x,,y,t 방향으로의 편미분을 $I_x, I_y, I_t$ 로 바꾸면.\n",
    "![](img/04_06.png)\n",
    "- 이를 행렬식으로 바꾸면 다음과 같이 바꿀수 있다.\n",
    "![](img/04_07.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d as conv\n",
    "\n",
    "\n",
    "def optical_flow(img1,img2):\n",
    "    kernel_x = np.array([[-1,0,1], [-1,0,1], [-1,0,1]])\n",
    "    kernel_y = np.array([[-1,-1,-1],[0,0,0], [1,1,1]])\n",
    "    \n",
    "    I_x = conv(img1,kernel_x, mode='same')\n",
    "    I_y = conv(img1,kernel_y, mode='same')\n",
    "    I_t = img2-img1\n",
    "\n",
    "    A = np.vstack((I_x.flatten(), I_y.flatten())).T\n",
    "    b = I_t.reshape(1,-1).T\n",
    "\n",
    "    # np.linalg : linear algbra\n",
    "    # pinv : 역행렬\n",
    "    nu = np.linalg.pinv(A).dot(b)\n",
    "    \n",
    "    return nu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16666667]\n",
      " [0.16666667]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "img1 = np.array([[0,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,0,0]])\n",
    "\n",
    "img2 = np.array([[0,0,0,0],\n",
    "                 [0,0,0,0],\n",
    "                 [0,0,1,0]])\n",
    "\n",
    "nu = optical_flow(img1,img2)\n",
    "print(nu)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# vector를 화살표로 그려주는 함수\n",
    "plt.quiver(1,1, nu[0], -nu[1]) # pyplot 에서는 아래가 양수이므로, 방향을 바꿔준다.\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lucas-Kanade method\n",
    "\n",
    " 위의 식(코드)로 했을 때의 문제는 이미지에 대해 하나의 vector만 나온다는 것이다. 따라서 위의 constraint를 기본으로 optical flow를 구하는 여러 알고리즘이 있는데, 그중에 대표적인 Lucas-Kanade 방법을 소개한다.\n",
    " \n",
    "- Lucas-Kanade 방법은 window를 씌워 각 window별로 flow vector를 구하는 것이다.\n",
    "![](img/05.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Optical Flow challenges\n",
    "\n",
    "\n",
    "### - Aperture problem\n",
    "\n",
    "조리개와 같이 작은 부분을 통해 본 움직임의 방향을 확정지을 수 없다. \n",
    "\n",
    "![](img/Aperture_Problem.gif)\n",
    "\n",
    "\n",
    "**Horn-Schunck method**\n",
    "![](img/10.png)\n",
    "\n",
    "---\n",
    "    \n",
    "### - Large displacement\n",
    "\n",
    "큰 움직임일 경우 에러가 크게 발생하는 문제\n",
    "\n",
    "Higher order term을 무시하고, first-order-taylor로 approximation하였기 때문에, short displacement가 전제된다. 따라서 Large displacement인 경우 에러가 많이 발생한다. \n",
    "\n",
    "\n",
    "**EpicFlow [CVPR2015]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. FlowNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataset : Flying Chairs\n",
    "\n",
    "당시 optical flow를 supervised learning으로 학습하기 위한 데이터가 부족했기 때문에 논문에서, 학습을 위해 만든 데이터셋을 같이 소개하였다.\n",
    "\n",
    "![](img/09.png)\n",
    "\n",
    "Flickr + chair\n",
    "\n",
    "![FlyingChairs](img/08.png)\n",
    "\n",
    "<center>출처 : https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model : Simple and Correlation\n",
    "\n",
    "논문에서는 두가지 모델을 제시한다. 각각 Simple 과 Correlation으로, 두 모델 모두 image 2장을 input으로 하여 optical flow를 예측하는 모델이다.\n",
    "\n",
    "### - FlowNetSimple\n",
    "\n",
    "Simple 모델은 두 Image를 Concat하여 Convolutional 모델에 넣어 학습한다. Input이 6채널(RGB 3채널이 두개)인 부분을 제외하고 일반적인 CNN의 모습과 같다.\n",
    "\n",
    "![flownet_simple](img/07_1.png)\n",
    "\n",
    "이론적으로는 네트워크가 충분히 크다면, optical flow를 잘 예측할 수 있지만. SGD를 통해 그 지점으로 갈 수 있을 지 확신할 수 없다. 그러므로 덜 일반적이지만, 유용한 hand-design적인 구조로 학습한다면 좀더 나은 성능을 보일 수 있을 것이다. \n",
    "\n",
    "\n",
    "### - FlowNetCorr\n",
    "\n",
    "Corr 모델은 각각의 image에 대해 feature를 어느정도 뽑아낸 후, 논문에서 제시하는 corr 연산을 통해 합쳐준다. 이후 다시 convolution layer를 반복하여 optical flow를 예측한다.\n",
    "\n",
    "\n",
    "![flownet_corr](img/07_2.png)\n",
    "\n",
    "\n",
    "\n",
    "#### - corr 연산 : \n",
    "\n",
    "corr 연산은 두 feature map을 비교를 하기 위한 연산이다.\n",
    "\n",
    "feature map f1의 x1와 feature map f2의 x2에 대한 corr 연산은 다음과 같이 정의 된다.\n",
    "\n",
    "![corr](img/11.png)\n",
    "\n",
    "corr 연산은 쉽게 말해, feature map을 또 다른 feature map을 kernel(혹은 filter)로 convolution 하는 연산을 말한다.\n",
    "(논문에서 muliplicative path comparisons라고 표현되어있다....)\n",
    "\n",
    "아래의 그림은 corr 연산을 표현한 것으로, 여기서 K는 patch의 사이즈를 뜻한다.\n",
    "모든 범위에 대해 연산을 수행하면 연산량이 너무 많으므로, 제한된 범위 D를 설정하여 해당 범위에 대해서만 연산을 수행한다.\n",
    "\n",
    "![corr_img](img/12.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "FlowNet은 unet과 같이, 앞에서 수행한 convolution layer들의 output들을 이후 upconvolution layer를 수행할 때 input과 concat하여 사용한다.\n",
    "\n",
    "![refinement](img/13.png)\n",
    "\n",
    "또한 upconvolution에 앞서, 작은 scale의 predict layer(flow_n)를 수행하고, 이 정보를 다음 upconvolution layer에 넘겨주어 사용한다.\n",
    "\n",
    "\n",
    "loss는 EPE(end point error) loss를 사용 (= L2 loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Result\n",
    "\n",
    "기존의 연산에 비해 성능은 조금 떨어지나, 연산 시간에 이점이 크다.\n",
    "\n",
    "![14](img/14.png)\n",
    "\n",
    "- ft : fine - tuning\n",
    "- v : variational loss 추가\n",
    "\n",
    "\n",
    "![15](img/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. FlowNet2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dataset : Flying Things 3D\n",
    "\n",
    "![](img/16.png)\n",
    "\n",
    "## 3.2 Stacking Networks\n",
    "\n",
    "- SOTA 방식들이 모두 iterative method인 점을 생각하여 network를 iterative 하게 해보자는 아이디어에서 출발\n",
    "\n",
    "![FlowNet2](img/07_3.png)\n",
    "\n",
    "- 두 네트워크를 쌓고 테스트하였을 때, Warped image를 넣어주면 성능이 증가한다는 결과를 고려하여 warped image를 input에 추가함 <br>warped image : 예측한 optical flow를 두번째 이미지에 bilinear한 방법으로 연산하여 만든 이미지\n",
    "\n",
    "- 네트워크를 쌓았을 때의 small displacement에 취약하여, small displacement를 따로 학습하는 모델을 추가. \n",
    "\n",
    "## 3.3 Result\n",
    "\n",
    "FlowNet2는 SOTA 방식과 비교하여 비슷한 성능을 내지만, 더욱 빠름.\n",
    "\n",
    "![](img/17.png)\n",
    "![](img/18.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
