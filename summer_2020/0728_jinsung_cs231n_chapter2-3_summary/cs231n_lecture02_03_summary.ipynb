{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS231n \n",
    "## Lecture 02, 03 summary\n",
    "김진성\n",
    "\n",
    "1. image classification\n",
    "2. Nearest Neighbor\n",
    "3. Linear classifier\n",
    "\n",
    "\n",
    "4. SVM loss\n",
    "5. softmax classifier\n",
    "6. Optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n",
    "\n",
    "Image Classification이란 input으로 이미지 정보를 받으면, 이 이미지가 어떤 이미지인지 분류해주는 행위를 말한다.\n",
    "\n",
    "\n",
    "\n",
    "~~~ python\n",
    "def classify_image(image):\n",
    "    # Some magic here?\n",
    "    return class_label\n",
    "~~~\n",
    "\n",
    "![01.png](img/01.png)\n",
    "\n",
    "\n",
    "---\n",
    "### - The Problems & Challenges\n",
    "\n",
    "computer에서의 image는 semantic한 정보가 아닌, 숫자의 행렬(matrix) 조합으로 구성되어 있다. 따라서 아래와 같은 경우 전혀 다른 숫자의 조합으로 표현되므로 image classification에 어려움을 준다.\n",
    "\n",
    "+ Viewpoint Variation (보는 각도)\n",
    "\n",
    "+ Illumincation (조명)\n",
    "\n",
    "+ Deformation(변형)\n",
    "\n",
    "+ Occlusion(은폐)\n",
    "\n",
    "+ Background Clutter(배경과 섞임)\n",
    "\n",
    "+ Intraclass variation(물체의 다양성)\n",
    "\n",
    "![02.png](img/02.png)\n",
    "\n",
    "---\n",
    "### - Attempts have been made\n",
    "\n",
    "가장 기초적으로 할 수 있는 방법으로, 규칙을 정하는 것이다.\n",
    "\n",
    "예를 들어 아래와 같은 고양이의 경우, edge detection을 통해 edge 정보를 얻은 뒤, 귀, 얼굴, 꼬리 등의 존재를 규칙기반으로 판단한 뒤, 고양이 임을 분류하는 방식이다. 이와 같은 방식은 비효율적이므로 사용하지 않는다.\n",
    "\n",
    "\n",
    "![03.png](img/03.png)\n",
    "\n",
    "---\n",
    "### - Data-Driven Approach\n",
    "##### 1. Collect a dataset of images and labels\n",
    "![04.png](img/04.png)\n",
    "\n",
    "##### 2. Use Machine Learning to train a classifier\n",
    "```python\n",
    "def train(images, labels):\n",
    "    # Machine learning\n",
    "    return model\n",
    "```\n",
    "\n",
    "##### 3. Evaluate the classifier on new images\n",
    "```python\n",
    "def predict(model, test_images):\n",
    "    # Use Model to predict labels\n",
    "    return test_labels\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Nearest Neighbor\n",
    "Image classification의 가장 기본적인 접근법이다.\n",
    "\n",
    "input 이미지가 들어왔을때, 미리 기억된 이미지들과 비교하여, 가장 비슷한 이미지를 선택하고 그 label로 분류하는 방법이다. \n",
    "\n",
    "- **train :** Just Memorize all data and labels, \n",
    "$O(1)$\n",
    "\n",
    "- **Predict :** Predict the label of the most similar training image,\n",
    "$O(N)$\n",
    "\n",
    "비슷한 이미지를 판별하기 위해, distance metric을 이용한다. distance metric에는 L1 distance와 L2 distance가 존재하며, distance 값이 작을 수록 해당 input 이미지와 비슷하다는 의미이다.\n",
    "\n",
    "- **Distance metric**\n",
    "    - L1 (Manhattan) distance\n",
    "$$d_1(I_1,I_2) = {\\displaystyle \\sum_{p} |{{I_1^p}-{I_2^p}|  }} $$\n",
    "    \n",
    "    - L2 (Euclidean) distance\n",
    "$$d_2(l_1,l_2) = \\sqrt{\\displaystyle \\sum_{p} {({I_1^p} - {I_2^p})}^2} $$\n",
    "\n",
    "<details><summary> How to calculate distance with L1 </summary>\n",
    "    \n",
    "![05.png](img/05.png)\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L1_distance(I1,I2):\n",
    "    return np.sqrt(np.sum(np.abs(I1-I2)))\n",
    "\n",
    "def L2_distance(I1,I2):\n",
    "    return np.sqrt(np.sum(np.square(I1,I2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighbor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n",
    "        # the nearest neighbor classifier simply remembers all the training data\n",
    "        self.Xtr = X\n",
    "        self.ytr = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        # lets make sure that the output type matches the input type\n",
    "        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n",
    "\n",
    "        # loop over all test rows\n",
    "        for i in range(num_test):\n",
    "            # find the nearest training image to the i'th test image\n",
    "            # using the L1 distance (sum of absolute value differences)\n",
    "            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n",
    "            min_index = np.argmin(distances) # get the index with smallest distance\n",
    "            Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - KNN(K-Nearest Neighbor)\n",
    "\n",
    "KNN은 Nearest Neighbor와 방식은 같으나, predict 과정에서 가장 가까운 이미지 하나를 선택하는 대신, 가장 가까운 K개의 이미지 중 **가장 많은 label (Mjority vote)**을 선택하는 방법이다.\n",
    "\n",
    "\n",
    "![06.jpg](img/06.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>2020 cs231n assignment1 KNN python code <br>\n",
    "    [assignment1\\cs231n\\classifiers\\k_nearest_neighbor.py]</summary>\n",
    "    \n",
    "```python\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "import numpy as np\n",
    "from past.builtins import xrange\n",
    "\n",
    "\n",
    "class KNearestNeighbor(object):\n",
    "    \"\"\" a kNN classifier with L2 distance \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the classifier. For k-nearest neighbors this is just\n",
    "        memorizing the training data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (num_train, D) containing the training data\n",
    "          consisting of num_train samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing the training labels, where\n",
    "             y[i] is the label for X[i].\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X, k=1, num_loops=0):\n",
    "        \"\"\"\n",
    "        Predict labels for test data using this classifier.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (num_test, D) containing test data consisting\n",
    "             of num_test samples each of dimension D.\n",
    "        - k: The number of nearest neighbors that vote for the predicted labels.\n",
    "        - num_loops: Determines which implementation to use to compute distances\n",
    "          between training points and testing points.\n",
    "\n",
    "        Returns:\n",
    "        - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "          test data, where y[i] is the predicted label for the test point X[i].\n",
    "        \"\"\"\n",
    "        if num_loops == 0:\n",
    "            dists = self.compute_distances_no_loops(X)\n",
    "        elif num_loops == 1:\n",
    "            dists = self.compute_distances_one_loop(X)\n",
    "        elif num_loops == 2:\n",
    "            dists = self.compute_distances_two_loops(X)\n",
    "        else:\n",
    "            raise ValueError('Invalid value %d for num_loops' % num_loops)\n",
    "\n",
    "        return self.predict_labels(dists, k=k)\n",
    "\n",
    "    def compute_distances_two_loops(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using a nested loop over both the training data and the\n",
    "        test data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (num_test, D) containing test data.\n",
    "\n",
    "        Returns:\n",
    "        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
    "          is the Euclidean distance between the ith test point and the jth training\n",
    "          point.\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train))\n",
    "        for i in range(num_test):\n",
    "            for j in range(num_train):\n",
    "                #####################################################################\n",
    "                # TODO:                                                             #\n",
    "                # Compute the l2 distance between the ith test point and the jth    #\n",
    "                # training point, and store the result in dists[i, j]. You should   #\n",
    "                # not use a loop over dimension, nor use np.linalg.norm().          #\n",
    "                #####################################################################\n",
    "                # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "               \tdists[i,j] = np.sqrt(np.sum( np.square(X[i,:] - self.X_train[j,:])))\n",
    "\n",
    "                # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return dists\n",
    "\n",
    "    def compute_distances_one_loop(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using a single loop over the test data.\n",
    "\t\t\n",
    "        Input / Output: Same as compute_distances_two_loops\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train))\n",
    "        for i in range(num_test):\n",
    "            #######################################################################\n",
    "            # TODO:                                                               #\n",
    "            # Compute the l2 distance between the ith test point and all training #\n",
    "            # points, and store the result in dists[i, :].                        #\n",
    "            # Do not use np.linalg.norm().                                        #\n",
    "            #######################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            dists[i, :] = np.sqrt(np.sum(np.square(X[i,:] - self.X_train), axis = 1))\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return dists\n",
    "\n",
    "    def compute_distances_no_loops(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using no explicit loops.\n",
    "\n",
    "        Input / Output: Same as compute_distances_two_loops\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train))\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Compute the l2 distance between all test points and all training      #\n",
    "        # points without using any explicit loops, and store the result in      #\n",
    "        # dists.                                                                #\n",
    "        #                                                                       #\n",
    "        # You should implement this function using only basic array operations; #\n",
    "        # in particular you should not use functions from scipy,                #\n",
    "        # nor use np.linalg.norm().                                             #\n",
    "        #                                                                       #\n",
    "        # HINT: Try to formulate the l2 distance using matrix multiplication    #\n",
    "        #       and two broadcast sums.                                         #\n",
    "        #########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "       \tdists = np.sqrt( (X**2).sum(axis=1)[:, np.newaxis] + (self.X_train**2).sum(axis=1) - 2*X.dot(self.X_train.T))\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return dists\n",
    "\n",
    "    def predict_labels(self, dists, k=1):\n",
    "        \"\"\"\n",
    "        Given a matrix of distances between test points and training points,\n",
    "        predict a label for each test point.\n",
    "\n",
    "        Inputs:\n",
    "        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
    "          gives the distance betwen the ith test point and the jth training point.\n",
    "\n",
    "        Returns:\n",
    "        - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "          test data, where y[i] is the predicted label for the test point X[i].\n",
    "        \"\"\"\n",
    "        num_test = dists.shape[0]\n",
    "        y_pred = np.zeros(num_test)\n",
    "        for i in range(num_test):\n",
    "            # A list of length k storing the labels of the k nearest neighbors to\n",
    "            # the ith test point.\n",
    "            closest_y = []\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Use the distance matrix to find the k nearest neighbors of the ith    #\n",
    "            # testing point, and use self.y_train to find the labels of these       #\n",
    "            # neighbors. Store these labels in closest_y.                           #\n",
    "            # Hint: Look up the function numpy.argsort.                             #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            closest_y = self.y_train[np.argsort(dists)[i,:k]]\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Now that you have found the labels of the k nearest neighbors, you    #\n",
    "            # need to find the most common label in the list closest_y of labels.   #\n",
    "            # Store this label in y_pred[i]. Break ties by choosing the smaller     #\n",
    "            # label.                                                                #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            indexs, counts = np.unique(closest_y, return_counts = True)\n",
    "            y_pred[i] = indexs[np.argmax(counts)]\n",
    "            \n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters\n",
    "\n",
    "모델을 만들거나 학습을 시작할 때, 사용자가 직접 세팅해주는 값 혹은 설정을 뜻한다.\n",
    "\n",
    "---\n",
    "#### - Hyperparameters in KNN\n",
    "\n",
    "KNN에서의 hyperparameter는 K값과, distance metric을 선택해 주어야 한다.\n",
    "\n",
    "K와 distance metric의 변화에 따른 KNN의 변화는 아래와 같다.\n",
    "\n",
    "<details><summary> K </summary>\n",
    "       \n",
    "![07.png](img/07.png)\n",
    "        \n",
    "</details>\n",
    "\n",
    "<details><summary> Distance metric </summary>\n",
    "        \n",
    "![08.png](img/08.png)\n",
    "        \n",
    "</details>\n",
    "    \n",
    "---\n",
    "#### - Hyperparameter tuning\n",
    "Hyperparameter는 **problem-dependent**하다. 따라서 좋은 hyperparameter를 고르기 위해서는 학습을 시도해 본 뒤 판단할 수 밖에 없다. 좋은 hyperparameter를 선택하는 과정을 hyperparameter tuning 이라고 한다.\n",
    "\n",
    "\n",
    "Hyperparameter tuning에는 주로 두가지 방법이 쓰인다. \n",
    "\n",
    "\n",
    "- Split data into **Train**, **Val**, and **Test**\n",
    "\n",
    "![09.png](img/09.png)\n",
    "<br>\n",
    "데이터를 Train, Validation, Test 데이터로 나누어 사용한다. 학습을 위한 데이터로 Train 데이터를 사용하며, 학습과정을 확인하기 위해 중간중간 Validation 데이터를 사용하여 테스트 한다. 학습이 완료되면, Test 데이터를 이용하여 최종 평가를 한다.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **Cross-Validation** : Split data into folds, try each fold as validation and average the results. Useful for small datasets.\n",
    "\n",
    "![10.png](img/10.png)\n",
    "\n",
    "두번째 방법은 Cross-Validation 방법이다. 데이터를 모으기 힘들어 데이터셋이 적은 경우 주로 사용하며, 데이터를 fold로 쪼개어 각 fold가 validation 역할을 돌아가며 학습하는 방법이다.\n",
    "\n",
    "![11.png](img/11.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> 2020 cs231n assignmnet1 Cross-Validation python code <br>\n",
    "    [assignment1\\knn.ipynb]</summary>\n",
    "    \n",
    "```python\n",
    "num_folds = 5\n",
    "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "\n",
    "X_train_folds = []\n",
    "y_train_folds = []\n",
    "\n",
    "X_train_folds = np.array_split(X_train, num_folds)\n",
    "y_train_folds = np.array_split(y_train, num_folds)\n",
    "\n",
    "k_to_accuracies = {}\n",
    "\n",
    "\n",
    "for k in k_choices:\n",
    "    if k not in k_to_accuracies:\n",
    "        k_to_accuracies[k] = []\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        X_train_fold = np.concatenate([X_fold for fold_num,X_fold in enumerate(X_train_folds) if fold_num != i])\n",
    "        y_train_fold = np.concatenate([y_fold for fold_num,y_fold in enumerate(y_train_folds) if fold_num != i])\n",
    "        X_valid_fold = X_train_folds[i]\n",
    "        y_valid_fold = y_train_folds[i]\n",
    "        num_valid = X_valid_fold.shape[0]\n",
    "        \n",
    "        classifier.train(X_train_fold, y_train_fold)\n",
    "        y_valid_pred = classifier.predict(X_valid_fold, k=k)\n",
    "        num_correct = np.sum(y_valid_pred == y_valid_fold)\n",
    "        \n",
    "        accuracy = float(num_correct) / num_valid\n",
    "        \n",
    "        k_to_accuracies[k].append(accuracy)\n",
    "    \n",
    "\n",
    "# Print out the computed accuracies\n",
    "for k in sorted(k_to_accuracies):\n",
    "    for accuracy in k_to_accuracies[k]:\n",
    "        print('k = %d, accuracy = %f' % (k, accuracy))\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem of k-Nearest Neighbor\n",
    "\n",
    "KNN은 아래와 같은 문제로 사용하지 않는다.\n",
    "\n",
    "- Distance metrics on pixels are not informative\n",
    "- Very slow at test time\n",
    "- Curse of dimensionality\n",
    "\n",
    "***==> Never Used***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier\n",
    "One of the simplest example of Parametric model\n",
    "\n",
    "### - Parametric Approach\n",
    "\n",
    "    어떠한 목적을 잘 할 수 있는 함수 f 와, 그 파라미터 W가 존재할 것이다 라는 가정하에 사용하는 접근법이다.\n",
    "    이때 f 를 목적함수(objective function) 이라고 한다.\n",
    "    목적함수 f가 1차 이하의 다항식으로 구성되어 있으면 linear 하다고 한다.\n",
    "\n",
    "\n",
    "![12.png](img/12.png)\n",
    "\n",
    "---\n",
    "\n",
    "### - Interpreting a Linear Classifier\n",
    "- **Algebraic Viewpoint**\n",
    "![13.png](img/13.png)\n",
    "\n",
    "- **Visual Viewpoint**\n",
    "![14.png](img/14.png)\n",
    "- **Geometric Viewpoint**\n",
    "![15.png](img/15.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Loss function\n",
    "    목적함수 f 를 평가하기위한 함수.\n",
    "    0에 가까울수록 (낮을 수록) 예측을 잘한다는 의미\n",
    "\n",
    "$$ L = \\frac{1}{N}\\sum_{i}{L_i(s_i, y_i )} $$\n",
    "\n",
    "$$ s = f(x_i ; W) $$\n",
    "- **Multiclass SVM loss** : \n",
    "\n",
    "    $ L_i = \\sum_{{j}\\neq{y_i}}{\\max(0, {s_j-s_{y_i}+ \\Delta})} $,\n",
    "    ($\\Delta$ : margin)\n",
    "    \n",
    "    margin 의 역할 : 정답을 잘 맞췄더라도, 정답의 Score 값($s_{y_i}$)이 다른 Score값($s_j$) 보다 margin 이상 크지 않으면 loss 를 발생시켜 학습할 수 있도록 해준다. 그 결과 좀더 안정적인 classifier가 될 수 있다.\n",
    "    \n",
    "<details><summary> bias, margin </summary>\n",
    "    \n",
    "![21.png](img/21.png)\n",
    "    \n",
    "</details>\n",
    "    \n",
    "    \n",
    "<br>\n",
    "\n",
    "- **Softmax Classifier** : \n",
    "    - Softmax function : $ P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum_{j}{e^{s_j}}} $\n",
    "    <br> : score 를 확률변수로 변환하는 역할\n",
    "    \n",
    "    <br>\n",
    "    - cross-entropy loss : $L_i = -logP(Y=y_i|X=x_i)$\n",
    "    \n",
    "    $ L_i = -log( \\frac{e^{f_{y_i}}}{\\sum_{j}{e^{f_j}}}) = -f_{y_i} + log \\sum_{j}{e^{f_j}}$\n",
    "\n",
    "<br>\n",
    "<details><summary> How to calculate SVM and Softmax loss </summary>\n",
    "\n",
    "- **SVM Loss**\n",
    "    \n",
    "![16.png](img/16.png)\n",
    "    \n",
    "- **Softmax loss**\n",
    "\n",
    "![17.png](img/17.png)\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### - Regularization\n",
    "\n",
    "loss function이 작아지는 방향으로 학습을 진행하면 특정 가중치가 너무 큰 값을 가질 수 있다. 이런 경우 train data에 대해서만 잘 작동하고, 일반적으로 성능이 떨어진다.\n",
    "이런경우를 overfitting 되었다고 한다.\n",
    "\n",
    "<img src=\"img/18.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "overfitting을 방지하는 방법으로, loss 함수에 페널티를 부여하여 특정 가중치만 너무 커지지 않도록 한다. 이때의 페널티를 regularization 이라고 한다.\n",
    "\n",
    "- $L(W) = \\frac{1}{N} \\sum^{N}_{i=1}{L_i(f(x_i, W),y_i)} + \\lambda R(W) $\n",
    "    - regularization strength : $\\lambda$\n",
    "    - regularization : $R(W)$\n",
    "\n",
    "#### - Regularization 종류\n",
    "\n",
    "- L2 regularization : $R(W) = \\sum_k \\sum_l W^{2}_{k,l}$\n",
    "- L1 regularization : $R(W) = \\sum_k \\sum_l |W_{k,l}|$\n",
    "- Elastic net(L1+L2) : $R(W) = \\sum_k \\sum_l {\\alpha} W^{2}_{k,l} + |W_{k,l}|$\n",
    "- Dropout\n",
    "- Batch normalization\n",
    "- Stochastic depth, fractional pooling, etc\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization (최적화)\n",
    "\n",
    "목적 함수에 맞는 parameter를 찾는 과정\n",
    "\n",
    "### - Gradient decent(경사하강법)\n",
    "\n",
    "$L(W)$ 를 최소화 하는 $W$를 찾기 위해, $W$의 값을 **neagtive gradient** 방향으로 update하는 방법\n",
    "\n",
    "- $W:= W - \\alpha\\frac{\\partial L(W)}{\\partial W}$\n",
    "    ($\\alpha$ : learning rate)\n",
    "\n",
    "#### - Stochastic Gradient Descent(SGD)\n",
    "\n",
    "    학습데이터의 개수(N)가 많아지면, L(W)를 구하는데 시간이 많이 걸림.\n",
    "    이러한 단점을 보안하기위해, 데이터를 minibatch로 나누어 N 사이즈를 줄여 학습.\n",
    "\n",
    "#### - learning rate\n",
    "    gradient decent를 사용할때 설정해야하는 hyperparameter로 적당한 크기의 learning rate를 찾아야 한다. 크기가 너무 큰 경우 발산할 가능성이 있고, 너무 작은 경우 학습하는데 오랜 시간이 걸린다. 또한 L(W)가 convex 하지 못한경우 local minimum에 빠질 수 있다.\n",
    "   \n",
    "![19.png](img/19.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_classifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        \n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iter=100,\n",
    "             batch_size=200, verbose=False):\n",
    "        \n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "        \n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "            \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            \n",
    "            batch_indexs = np.random.choice(num_train,batch_size)\n",
    "            X_batch = X[batch_indexs]\n",
    "            y_batch = y[batch_indexs]\n",
    "            \n",
    "            #eveluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # update W\n",
    "            self.W -= learning_rate * grad\n",
    "            \n",
    "            if verbose and it%100 == 0:\n",
    "                print('iteration %d / %d : loss %f' %(it, num_iters, loss))\n",
    "        \n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        scores = X.dot(self.W)\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "    \n",
    "class LinearSVM(Linear_classifier):\n",
    "    \n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        Loss = 0.0\n",
    "        dW = np.zeros_like(self.W)\n",
    "        \n",
    "        num_train = X_batch.shape[0]\n",
    "        \n",
    "        # calculate svm loss\n",
    "        scores = X_batch.dot(self.W)\n",
    "        correct_class_scores = scores[range(num_train), y_batch][:, np.newaxis] # (N,1) shape\n",
    "        \n",
    "        margin = np.maximum(0, scores - correct_class_scores + 1)\n",
    "        margin[range(num_train),y_batch] = 0\n",
    "        \n",
    "        Loss = np.sum(margin)\n",
    "        Loss /= num_train\n",
    "        Loss += reg*np.sum(self.W*self.W)\n",
    "        \n",
    "        # calculate svm gradient\n",
    "        margin_count = np.zeros(scores.shape)\n",
    "        margin_count[margin>0] = 1\n",
    "        margin_count[range(num_train), y_batch] = -np.sum(margin_count, axis=1)\n",
    "        \n",
    "        dW = X_batch.T.dot(margin_count)\n",
    "        \n",
    "        dW /= num_train\n",
    "        dW += 2*reg*self.W\n",
    "        \n",
    "        return Loss, dW\n",
    "        \n",
    "class Softmax(Linear_classifier):\n",
    "    \n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        Loss = 0.0\n",
    "        dW = np.zeros_like(self.W)\n",
    "        \n",
    "        num_train = X_batch.shape[0]\n",
    "        \n",
    "        # calculate cross-entropy loss\n",
    "        scores = X_batch.dot(self.W)\n",
    "        softmaxs = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "        Loss = np.sum( -np.log(softmaxs[range(num_train),y_batch]))\n",
    "        Loss /= num_train\n",
    "        Loss += reg *np.sum(self.W * self.W)\n",
    "        \n",
    "        # calculate gradients\n",
    "        softmaxs[range(num_train),y_batch] -= 1\n",
    "        dW = X_batch.dot(softmaxs)\n",
    "        dW /= num_train\n",
    "        dW += reg*2*self.W\n",
    "        \n",
    "        return Loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SVM loss gradient\n",
    "- $s_j = w_j x_i$\n",
    "- $s_{y_i} = w_{y_i} x_i$\n",
    "- $L_i = \\sum_{j\\neq y_i}{max(0,s_j - s_{y_i}+\\Delta)}$\n",
    "에서\n",
    "<br><br>\n",
    "\n",
    "\n",
    "$\\nabla_{w_j}{L_i} = \\nabla_{w_j}\\sum_{j\\neq y_i}{max(0,s_j - s_{y_i}+\\Delta)}= 1(s_j - s_{y_i} + \\Delta > 0) \\nabla_{w_j}(s_j - s_{y_i} + \\Delta) $\n",
    "\n",
    "$\\nabla_{w_j}{s_j} = \\nabla_{w_j}{w_j x_i} = x_i$ 이므로\n",
    "\n",
    "$\\therefore \\nabla_{w_j}{L_i} =  1(s_j - s_{y_i} + \\Delta > 0) x_i$\n",
    "\n",
    "마찬가지로, \n",
    "\n",
    "\n",
    "$\\nabla_{w_{y_i}}{L_i} = \\nabla_{w_{y_i}}\\sum_{j\\neq y_i}{max(0,s_j -s_{y_i}+\\Delta)}= \\sum_{j \\neq y_i}{(1 \\cdot (s_j - s_{y_i} + \\Delta > 0)) \\cdot \\nabla_{w_{y_i}}(s_j - s_{y_i} + \\Delta)} $\n",
    "\n",
    "$\\nabla_{w_{y_i}}{s_{y_i}} = \\nabla_{w_{y_i}}{w_{y_i} x_i} = x_i$ 이므로\n",
    "\n",
    "$\\therefore \\nabla_{w_{y_i}}{L_i} = -x_i \\cdot \\sum_{j \\neq y_i}{(1 \\cdot (s_j - s_{y_i} + \\Delta > 0))}$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Calculate cross entropy loss gradient\n",
    "- $L = -s_{y} + ln(\\sum_{j}{e^{s_j}}) $ 에서\n",
    "\n",
    "$\\nabla_{s_i}{L} = -\\nabla_{s_i}s_{y} + \\nabla_{s_i}ln(\\sum_{j}{e^{s_j}})$\n",
    "\n",
    "$= \\frac{\\nabla_{s_i}(\\sum_{j}{e^{s_j}})}{\\sum_{j}{e^{s_j}}} - \\nabla_{s_i}{s_y} $\n",
    "\n",
    "$= \\frac{e_{s_i}}{\\sum_{j}{e^{s_j}}} - \\nabla_{s_i}{s_y} $\n",
    "\n",
    "$= softmax_i - \\nabla_{s_i}{s_y} $\n",
    "\n",
    "$=\n",
    "\\begin{cases}\n",
    "    softmax_i - 1 & {y=i} \\\\\n",
    "    softmax_i & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Chain rule에 의해\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial W_j}= \\frac{\\partial L}{\\partial s_i} \\times \\frac{\\partial s_i}{\\partial W_j} $\n",
    "\n",
    "$= x_i \\times \\begin{cases}\n",
    "    softmax_i - 1 & {y=i} \\\\\n",
    "    softmax_i & \\text{otherwise}\n",
    "\\end{cases} $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- [cs231n/classification](cs231n.github.io/)\n",
    "- [cs231n/linear-classify](https://cs231n.github.io/linear-classify/)\n",
    "- [cs229 lecture-note3](http://cs229.stanford.edu/notes/cs229-notes3.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
