{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyxXxMPCE4X0"
      },
      "source": [
        "# 문자 단위 RNN\n",
        "  - 모든 시점의 입력에 대해서 모든 시점에 대해서 출력을 하는 다대다 RNN 구현\n",
        "  - 문자 단위란?\n",
        "    - word-level이 아니라 character-level로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWakCoFwAv2-"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_rE3fLXFaY9"
      },
      "source": [
        "### 문자 시퀀스 apple을 받으면 -> pple!를 출력하는 RNN\n",
        "  - input data와 label data에 대해서 문자 집합을 만든다. (중복 제거)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pscOb6MIA1me",
        "outputId": "48a2cf64-9f4c-437c-a317-b8cc4d13b594"
      },
      "source": [
        "#문자 집합을 만든다\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str))) #set 중복 제거\n",
        "print(char_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', 'a', 'e', 'l', 'p']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWFx_1HNBH7b",
        "outputId": "e2f8516c-9731-4c34-800c-36c70ffba588"
      },
      "source": [
        "vocab_size = len(char_vocab)\n",
        "print(f'문자 집합의 크기:{vocab_size}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기:5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7275557gDffw"
      },
      "source": [
        "#원 핫 벡터를 사용할 것이므로 입력의 크기 = 문자 집합의 크기\n",
        "input_size = vocab_size\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "lr = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGxcPv1fDwSs",
        "outputId": "4d7a09cb-5d57-436d-bb8d-2c7c4d30e08a"
      },
      "source": [
        "#문자에 고유한 정수 인덱스 부여\n",
        "char_to_index = dict((c,i) for i,c in enumerate(char_vocab))\n",
        "print(char_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peOjJPWlD3Yt",
        "outputId": "ffa41174-584f-4946-870b-1bd4c484fa54"
      },
      "source": [
        "index_to_char = {} #나중에 예측 결과를 다시 문자 시퀀스로 보기 위해서 반대로도 만들어 놓음\n",
        "for key,value in char_to_index.items():\n",
        "  index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6kwGZzzKFRz",
        "outputId": "8de51ba6-28ee-4f3c-d42c-7805617b421a"
      },
      "source": [
        "print(input_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apple\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMs7u4CMKLu7",
        "outputId": "00ac9cf3-e4cf-42dc-808e-7989f05818fc"
      },
      "source": [
        "char_to_index['a']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 417
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf5GSamMEUiV",
        "outputId": "3e2ee12f-328a-4ff6-e89a-c8fa23271c14"
      },
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data) #a,p,p,l,e\n",
        "print(y_data) #p,p,l,e,!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YEaBbdrFRZ0",
        "outputId": "011c060a-598e-4ae7-cf38-be380e4fede1"
      },
      "source": [
        "#파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력 받음. 그렇기 떄문에 배치 차원을 추가해준다\n",
        "#배치 차원 추가\n",
        "#텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있음\n",
        "\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data) #a, p, p, l, e\n",
        "print(y_data) #p, p, l, e, !"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0eZiAgOwxL"
      },
      "source": [
        "### expand_dims로도 차원을 추가해줄 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sD3860hOt2S",
        "outputId": "006751a4-c282-48e3-e6c3-aa3012fa7f92"
      },
      "source": [
        "x_data_unsqueeze = np.expand_dims(x_data,0)\n",
        "print(x_data_unsqueeze)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1 4 4 3 2]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_1mKGUqcSMU",
        "outputId": "6470bb6a-785d-4a4f-cea2-fdf73b4fb756"
      },
      "source": [
        "torch.LongTensor(x_data).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 4, 4, 3, 2]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 421
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCNYXj4KHTmx",
        "outputId": "2d4e05ac-2a3a-40c3-d990-1ea1f75d6226"
      },
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data] #총 행렬의 크기 5x5로 만들어라 \n",
        "print(x_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMCyGZfEPyB9",
        "outputId": "852c41e9-7d70-4716-e2a6-f9cd2708d3a8"
      },
      "source": [
        "np.eye(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 423
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxQBKr0DKqCt",
        "outputId": "dedf31b0-f811-4810-e71a-042b19f7ec42"
      },
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print(f'훈련 데이터의 크기:{X.shape}')\n",
        "print(f'레이블의 크기:{Y.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기:torch.Size([1, 5, 5])\n",
            "레이블의 크기:torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSYCqgT6LuIE"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(Net,self).__init__()\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,batch_first=True) #RNN 셀 구현\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size,output_size,bias=True) #출력층 구현\n",
        "\n",
        "  def forward(self,x):\n",
        "    x,_status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKk9dmriMkPS"
      },
      "source": [
        "net = Net(input_size,hidden_size,output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrScTbWrNIui",
        "outputId": "b6b1e06c-edb2-4155-cad5-916611e2b301"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "oprimizer = optim.Adam(net.parameters(),lr)\n",
        "\n",
        "outputs = net(X)\n",
        "print(outputs)\n",
        "print('')\n",
        "print(outputs.shape) #배치 차원, 시점(timesteps), 출력의 크기 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2935,  0.3544,  0.3451,  0.5131,  0.3802],\n",
            "         [-0.1107,  0.3920,  0.3779,  0.5646,  0.2775],\n",
            "         [-0.0144,  0.3896,  0.4192,  0.5535,  0.3788],\n",
            "         [-0.0586,  0.6031,  0.2736,  0.2566,  0.2084],\n",
            "         [-0.0397,  0.5191,  0.4932,  0.5006,  0.4519]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "torch.Size([1, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfvtUcrVP_MB",
        "outputId": "d93d1a27-61dc-4801-81b5-14f72fa909ef"
      },
      "source": [
        "print(outputs.view(-1,input_size).shape) #2차원 텐서로 변환 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm_avgcbAdRb",
        "outputId": "18957681-61b1-405b-9359-2e235ca5434c"
      },
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape) #정확도를 측정할때는 이걸 펼쳐서 계산할 예정이니까\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n",
            "tensor([[4, 4, 3, 2, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXmoG1AvBRH2"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(),lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkH69G5DBaiv",
        "outputId": "b00b260c-03ac-407e-d803-a55734981e9e"
      },
      "source": [
        "for i in range(10):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs.view(-1,input_size), Y.view(-1)) #view를 하는 이유 Batch차원 제거를 위해\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  #모델이 실제 어떻게 예측했는지를 확인하기 위한 코드\n",
        "  result = outputs.data.numpy().argmax(axis=2)\n",
        "  print(result)\n",
        "  result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "  print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3 3 3 1 1]]\n",
            "0 loss:  1.6705913543701172 prediction:  [[3 3 3 1 1]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  lllaa\n",
            "[[4 4 3 4 4]]\n",
            "1 loss:  1.4243745803833008 prediction:  [[4 4 3 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplpp\n",
            "[[4 4 3 4 4]]\n",
            "2 loss:  1.2227948904037476 prediction:  [[4 4 3 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplpp\n",
            "[[4 4 3 2 4]]\n",
            "3 loss:  1.0377991199493408 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "[[4 4 3 2 4]]\n",
            "4 loss:  0.858181357383728 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "[[4 4 3 2 4]]\n",
            "5 loss:  0.7077634334564209 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "[[4 4 3 2 4]]\n",
            "6 loss:  0.570701539516449 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "[[4 4 3 2 0]]\n",
            "7 loss:  0.43255677819252014 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "[[4 4 3 2 0]]\n",
            "8 loss:  0.3455711007118225 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "[[4 4 3 2 0]]\n",
            "9 loss:  0.24282188713550568 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnEVUbRHFE5k"
      },
      "source": [
        "## 더 많은 문장으로 했을때는?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3lLVqy8B55x"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFw4-K8LFD2q"
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LNbk0w2Piwb",
        "outputId": "63c26d66-e1e6-401a-907d-5859fbce393d"
      },
      "source": [
        "len(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 434
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFXHl7kmFDrf",
        "outputId": "8931ac98-efc4-46a0-82bc-a60a9f1bc786"
      },
      "source": [
        "char_set = list(set(sentence)) #중복을 제거한 문자 집합\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} #각 문자에 정수 인코딩\n",
        "print(char_dic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 0, 'a': 1, 'w': 2, 'r': 3, '.': 4, 'u': 5, 'y': 6, \"'\": 7, 'k': 8, ',': 9, 'g': 10, 'c': 11, 'n': 12, 'h': 13, 'b': 14, 's': 15, 'd': 16, 'o': 17, 'l': 18, 'e': 19, 'f': 20, 't': 21, 'p': 22, ' ': 23, 'm': 24}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmPv2DXQFbz1",
        "outputId": "aa76c352-5bcd-4c3c-8e21-e86deda0086a"
      },
      "source": [
        "dic_size = len(char_dic)\n",
        "print(f'문자 집합의 크기:{dic_size}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기:25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O2rWPS4F5ms"
      },
      "source": [
        "hidden_size = dic_size\n",
        "sequence_length = 10 #임의 숫자 \n",
        "lr = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87g-5zqcGkFs",
        "outputId": "eb7e6011-1965-43b2-c4de-75ab401d8ef9"
      },
      "source": [
        "#10의 단위로 샘플들을 잘라서 데이터를 만듬\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0,len(sentence) - sequence_length):\n",
        "  x_str = sentence[i: i + sequence_length]\n",
        "  y_str = sentence[i+1: i + sequence_length + 1]\n",
        "  print(i, x_str, '->', y_str)\n",
        "\n",
        "  x_data.append([char_dic[c] for c in x_str]) # x str to index\n",
        "  y_data.append([char_dic[c] for c in y_str]) # y str to index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQJVuy9zHCBc",
        "outputId": "879a5dbc-488e-47ab-c4a6-9083b1960580"
      },
      "source": [
        "print(x_data[0]) #if you wan에 해당\n",
        "print(y_data[0]) # f you want에 해당\n",
        "#한 칸씩 쉬프트로 된 시퀀스 \n",
        "#<sos>?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 20, 23, 6, 17, 5, 23, 2, 1, 12]\n",
            "[20, 23, 6, 17, 5, 23, 2, 1, 12, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iudy3LC3HaJT"
      },
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] #x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDBAt9HtH1VR",
        "outputId": "172442dc-f1df-4ca2-8092-4b1b2eba53d4"
      },
      "source": [
        "print(f'훈련 데이터의 크기:{X.shape}')\n",
        "print(f'레이블의 크기:{Y.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기:torch.Size([170, 10, 25])\n",
            "레이블의 크기:torch.Size([170, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go6lmr6mIBk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7348547-078c-498a-b6ab-137944656f96"
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzpVVwWVUj2S"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqLQVgpzW61-"
      },
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLJu01YMW-2y"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t-vfyyhXAMZ",
        "outputId": "cdc50c36-7e17-4c29-f3c0-7ce03022486f"
      },
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOh7E0hZXDhR",
        "outputId": "1dee8939-bdb0-436f-c5d2-d7fe53e0786c"
      },
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1700, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGDmsAlVXE9j",
        "outputId": "615ca482-9195-46f4-c4dc-a587c3eb8352"
      },
      "source": [
        "for i in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esbssssssssssssssssbsssssssssssssbsssssssbssssbsbssssssbbsssssssbbbssssssssssssssssbssssssssssbssssssbssssssssbbssssssssbbssssssbssbbssssssbsssssssssbsssbbbssssssssbssbssssssbssss\n",
            "                                                                                                                                                                                   \n",
            " h,hdhnhnhh,hnhnp,,hnhtp,,,eshnhtuhhh,m,h,h,mmm,edhn,mem,m,h,,,dmnheh,,,,,,hehnhn,hhtuh,hemnhd,mnhnh,,,,m,,hnhtutuheh,heh,m,hehem,,m,,mh,,,emnhdhhnhemm,h,m,sh,,m,m,h,heh.hnhnh,,,,\n",
            "  hot tot o ouh  t o otow t ton t t tot   t tgot ho  h o s o hh   o oe     tuo onot t t t  oo  gto  ho  woton t t t o   t r o  he      n goo  to hton t t t t t w t  ht ouoton     \n",
            "    r t t    t t t  t t t e t t t t e t      t t t          t t  n   t        t t t t    n o    t   t   w t t t t t   t t t    t         en    o  t t t t e t t e t      t t t     \n",
            "mmtorh lntltl l toeotleoe etleele ltleleelle leo et etoetle elnnln e dntl t'eoe elb et nletllenle erendowotoe e ltmntln emltlnrontomnwtotlenlnlnrlt erlnlotoe loetltolltltle erwon'\n",
            "mttntnr itmtodoetrtrtotoentodndor g i em ,g  tee r r i etmltg g dntntgnt ,tmtotoeoi m inertoem mmem m dndototor i dndod m mtomtg toe tt mto tntotpdor indodptntpepitrtgtgtoroiei rn\n",
            "e tntototoe oto otoototndodotodotoeod etoetdndoe totoe e tte  t ene odooneohtotod'todod e e d   toe e dndndodod tododndne eto p  d dnet rte endod tod toend dotodotoile odotod doep\n",
            "e   todod odod   deod d  md d dedhe    tod d th  dod  h   todct e  tod'ec t dod d'dhd d e  coe idod d t t t'd d dod t t   ttoe d     rdhdod t t t dod dhe t d dld d  ddd d dod dod \n",
            "e   t d   hs s  t'  t d  ms d re  e  t t scd  e  do   h   torch    t  ' ' tpd t re  d iid    e  t s d thd d'res t sh   e   t   ie     thr     t r r   dh  tes dm  st de  i d   d   \n",
            "e   stdo tot rt tmt t th pg d rtt s  t t sch gt  to   h   t tc  e  t t' met d t tm  t  te       t   t   t t'r t t sht te   t  ste     t tp ee t r t   t   ses t   rt te t  to  t   \n",
            "e t  tto tht tb tmtht thnpg rotbt t  p t tpo ge  tot    s t tth ge t t' t't tmt t'  t  lge t  l thnlt     ton t tot tmn    to lh tm   tonphng t tpt  st   tm  to   t ttet tto sthnm\n",
            "gmto tfo tototlu mt t w npg tot'u t  p totpf ge  tht   e  tonphtge t to tot t tonlu t tlg  t  l tonmt to  ton t tot t n    to l  t    tonpongothn thnmtht tm  gon  t ttet nto pthnp\n",
            "gmto tton  totle  t w t nmt tot'ehtoem totluntde t t   e  tonchnge t tontot t todme to cgmtt n etoe t tod ton t tot t t e  toe   t    toncong ton t e t d tm  t n mt g tb  ton thnp\n",
            "geto ttoot tod'eete t d emg ton u doet t etuntee t d   e  toec nle t doet t t tod't t ukgmtt e etoe t t d doe t d d t d e  toe  et e  t e ond tod t e t d d   tee  t g e'u toe t e \n",
            "geto  doot tod'u tl d d ep, tor'tod et d eluette t d   e  toepfnde todoet a d tod'tod's gmtthe etoe d dor dod d doe d d e  toe  et e  toe ong tod t e t e d   d'e  t d d'fodoe the \n",
            "geto  aont todpoitg t a ep, tor'todoet d eluotue tor   e  toe ongectodool aod tor'tod's gm the  tod a aor dor d doe d s e  toe het e  toetend tor the t d e   d'e  tst d'fodoe the \n",
            "leto  aost aorpuil, tha ep, tor'tod et t epuople t r   e  toeponlest donl aod aor'tod's gn the  tod a anr aor t t e d the  toe h t e  toolond tor the the e s t'e    ty 'f doe the \n",
            "leto  to   to pusll tha ep, tor'todo m tpepe pte thr'  e  ta pfnlect donloa d aor't tps gn the  tas t tns aor t t s dhthe  ta  h the  taolong tor the t e e s tps s sty pf toe th p\n",
            "lnto  to   ao buspl tha  m, tor'thth m tp penple thd   e  ta pfrlest donlotnd aor't tpsign the  ta  t tns aor t t t rht e  te ch the  ta pong tor the t d t s tps s  to of toe th  \n",
            "ln o  to s do busl, tha  m, tormthd  m tp tenple th  t e  to conle t aooloand aor't t'smgn them tanmt tns dor t a t dht e  tench the  ta pong tor the thd     dp  s  to of aoe th  \n",
            "ln o stons do butly ana  m, won'thao m tp penplemthd them to conle t aooloand dor't dssign the  tanmt an  dor t aat d t em tench ohem to long tor the t d e s dms s  g  of doe th t\n",
            "ln o  do t donbutly t an m, won'thaoem tpepenpgemtod them to corle t aoodotnd dor't aosign them tanks ans dor t aht d them te mh o em to long tor the t d e s dms s sdy of dee t  m\n",
            "lnto  tort tonbutld tnanem, don't aoem teepeoplertod them toecorlens aood tnd don't aosign them tenks ans dor t aet d them teemh o em to long tor the t d e s dms s sty of dee t em\n",
            "l tor tort tonbusld tndnep, don't aoem teepeoplertod them toecollens aood tnd don't acsigemthem tatk, ans dor , aut d the  toech ohem toglong tor the t dme s dms s  ty of dee t  m\n",
            "lntor tont tonbusld t ahep, dor't aoem tpepeopge tod the  toolollent aood anr dor't aosign them tosks ans dor , aat r the  toeph ohem toglong tor the t d e i dpm st ty of toe t sm\n",
            "lntorttant tonbusld tnrhep, don't arem tp peoplectod them to collent aood ang don't aosign them tosks ans tor , aat rethem toeph ohem togcong tor the t dmd i dpm ss ty of toe t st\n",
            "lnton tant toncusld anshep, don't arum tp people too them teacollent aood ang don't aosign them tosks ans torkt aut dashem toaph them togcong tor the t d e i ipm ns ty of toe tnns\n",
            "ln onhtant tonbusld anrhep, don't arem ap peoplecth ethem toacollect aond ang don't ansign them tosks and tork, aut dothem toach them togcong tor the tndkd i dmm nsityuof toe tnnm\n",
            "ln o  tant to lusld ansh l, don't arum ap peoplecth ethem teaco lect aood and don't assisn them tosks and dork, aut dathem teaph them to long for the tndl  i dmm nsityuof toe t sm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8HwXh8VXXl6"
      },
      "source": [
        "## Embedding 사용!!\n",
        "  - embedding layer를 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MilZiJRIXY-Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHpegg6oZKyk",
        "outputId": "6d1908db-5758-44bd-af2e-7327bf7c7e49"
      },
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()\n",
        "vocab = list(set(sentence))\n",
        "print(vocab)\n",
        "#우리가 만들 RNN은 \"Repeat is the best medicine for\"을 입력 받으면 \"is the best medicine for memory\"를 출력하는 RNN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['for', 'Repeat', 'best', 'the', 'is', 'memory', 'medicine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBkUSgsAZVS8",
        "outputId": "3b66afd4-6344-4525-9509-461d57631acb"
      },
      "source": [
        "word2index = {tkn:i for i,tkn in enumerate(vocab,1)} #단어에 고유한 정수 부여\n",
        "word2index['<unk>']=0\n",
        "print(word2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'for': 1, 'Repeat': 2, 'best': 3, 'the': 4, 'is': 5, 'memory': 6, 'medicine': 7, '<unk>': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKi_bE3v_d63",
        "outputId": "1fe64bc3-7f4b-4ca9-dad7-b2240e59027f"
      },
      "source": [
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'for', 2: 'Repeat', 3: 'best', 4: 'the', 5: 'is', 6: 'memory', 7: 'medicine', 0: '<unk>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-fhlcwHbP2n"
      },
      "source": [
        "#우리가 사용할 word2index단어장\n",
        "def build_data(sentence,word2index):\n",
        "  encoded = [word2index[token] for token in sentence] #각 문자를 정수로 변환\n",
        "  input_seq, label_seq = encoded[:-1],encoded[1:] #입력 시퀀스와 레이블 시퀀스를 분리\n",
        "  input_seq = torch.LongTensor(input_seq).unsqueeze(0) #배치 차원 추가\n",
        "  label_seq = torch.LongTensor(label_seq).unsqueeze(0) #배치 차원 추가\n",
        "  return input_seq,label_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PENpv29NlA-z",
        "outputId": "756fa3e7-7f78-4d3d-bd98-67f1909e3f14"
      },
      "source": [
        "X,Y = build_data(sentence,word2index)\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2, 5, 4, 3, 7, 1]])\n",
            "tensor([[5, 4, 3, 7, 1, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpMgwFtflIo5"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, \n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) \n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # 4. view를 통해서 배치 차원 제거\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf7R8LM25pPe"
      },
      "source": [
        "vocab_size = len(word2index) \n",
        "input_size = 5      \n",
        "hidden_size = 20   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGso6pNK9SGm"
      },
      "source": [
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "loss_function = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpkAsxXf9HEk",
        "outputId": "f0dff700-be34-45b3-ac18-f9db1d7806e0"
      },
      "source": [
        "#(시퀀스의 길이, 은닉층의 크기)\n",
        "output = model(X)\n",
        "print(output)\n",
        "print('')\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2843,  0.4345, -0.1295,  0.0404, -0.1824, -0.0354, -0.0799, -0.0787],\n",
            "        [ 0.4519,  0.3138, -0.0463, -0.2250, -0.1907, -0.2845, -0.1057,  0.3123],\n",
            "        [-0.0616,  0.5515, -0.4119,  0.4048, -0.2292, -0.1598, -0.3653, -0.4435],\n",
            "        [ 0.1207,  0.5257, -0.1004, -0.0414, -0.3587, -0.1495, -0.0245, -0.0326],\n",
            "        [ 0.5649,  0.2861, -0.1643, -0.0426, -0.1589, -0.2638, -0.2193,  0.0693],\n",
            "        [ 0.0561,  0.0034,  0.1165, -0.0520, -0.1553, -0.4532, -0.1832, -0.0336]],\n",
            "       grad_fn=<ViewBackward>)\n",
            "\n",
            "torch.Size([6, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVaaBqqt9PW_"
      },
      "source": [
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LKFzpaE_McQ",
        "outputId": "5946ca36-bded-4cdd-be18-c47b92c6e740"
      },
      "source": [
        "for step in range(201):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 기록\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[01/201] 2.0533 \n",
            "Repeat for <unk> for for <unk> Repeat\n",
            "\n",
            "[41/201] 1.3735 \n",
            "Repeat is medicine best medicine for memory\n",
            "\n",
            "[81/201] 0.8015 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.3995 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.2091 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1274 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52XoIHY0CgSk"
      },
      "source": [
        "Reference: https://wikidocs.net/64765 를 참조하였습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO4B5q9bevpz"
      },
      "source": [
        "### word based or character based?\n",
        "  - word based: higher accuracy and lower computational cost than character based LMs\n",
        "  - however, character based RNN LMs better model languages with a rich morphology(형태론) such as Finish, Turkish, Russian etc. Using word based RNN LMs to model such languages is difficult if possible at all and is not advised\n",
        "  - out of vocabulary word, because can only handle those seen words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iS5uTAPfW0m"
      },
      "source": [
        "형태론 - 단어의 어형(語形) 변화를 다루는 문법의 한 분야이다.\n",
        "\n",
        "한 언어에서 형태소들이 결합하여 낱말을 형성하는 체계 또는 규칙으로, 형태소(morpheme) 및 낱말을 기본 단위로 한다."
      ]
    }
  ]
}